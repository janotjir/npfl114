title: NPFL114, Lecture 14
class: title, langtech, cc-by-sa

# Speech Synthesis,<br>External Memory Networks

## Milan Straka

### May 15, 2023

---
section: WaveNet
# WaveNet

Our goal is to model speech, using a convolutional auto-regressive model
$$P(→x) = ∏_t P(x_t | x_{t-1}, …, x_1).$$

~~~
![w=80%,h=center](wavenet_causal_convolutions.svgz)

---
# WaveNet

However, to achieve larger receptive field, we utilize **dilated** (or **atrous**) convolutions:

![w=100%,v=middle](wavenet_dilated_convolutions.svgz)

---
# WaveNet – Output Distribution

## Output Distribution

WaveNet generates audio with 16kHz frequency and 16-bit samples.

~~~
However, classification into $65\,536$ classes would not be efficient. Instead,
WaveNet adopts the $μ$-law transformation, which passes the input samples in
$[-1, 1]$ range through the $μ$-law encoding
$$\sign(x)\frac{\log(1 + 255|x|)}{\log(1 + 255)},$$
and the resulting $[-1, 1]$ range is linearly quantized into 256 buckets.

~~~
The model therefore predicts each samples using classification into 256 classes,
and then uses the inverse of the above transformation on the model predictions.

<audio controls style="width: 49%"><source src="https://upload.wikimedia.org/wikipedia/commons/f/f4/Larynx-HiFi-GAN_speech_sample.wav"></audio>
<audio controls style="width: 49%"><source src="https://upload.wikimedia.org/wikipedia/commons/d/da/Mu-law_audio_demo.flac"></audio>

---
# WaveNet – Architecture

![w=68%,h=center](wavenet_block.svgz)

~~~
The outputs of the dilated convolutions are passed through the _gated activation
unit_:
$$→z = \tanh(⇉W_f * →x) ⊙ σ(⇉W_g * →x).$$

---
# WaveNet

## Global Conditioning
Global conditioning is performed by a single latent representation $→h$,
changing the gated activation function to
$$→z = \tanh(⇉W_f * →x + ⇉V_f→h) ⊙ σ(⇉W_g * →x + ⇉V_g→h).$$

~~~
## Local Conditioning
For local conditioning, we are given a time series $→h$, possibly with a lower
sampling frequency. We first use transposed convolutions $→y = f(→h)$ to match resolution
and then compute analogously to global conditioning
$$→z = \tanh(⇉W_f * →x + ⇉V_f * →y) ⊙ σ(⇉W_g * →x + ⇉V_g * →y).$$

---
# WaveNet

The original paper did not mention hyperparameters, but later it was revealed
that:
- 30 layers were used

~~~
  - grouped into 3 dilation stacks with 10 layers each
~~~
  - in a dilation stack, dilation rate increases by a factor of 2, starting
    with rate 1 and reaching maximum dilation of 512
~~~
- filter size of a dilated convolution is 2 (and extended to 3 in Parallel
  WaveNet)
~~~
- residual connection has dimension 512
~~~
- gating layer uses 256+256 hidden units
~~~
- the $1×1$ convolutions in the output step produce 256 filters
~~~
- trained for $1\,000\,000$ steps using Adam with a fixed learning rate of 2e-4

---
# WaveNet

![w=85%,h=center](wavenet_results.svgz)

---
section: GLUs
class: tablewide
style: table { line-height: 1.0 }
# Gated Activations in Transformers

Similar gated activations seem to work the best in Transformers, in the FFN
module.

~~~
| Activation Name | Formula | $\operatorname{FFN}(x; ⇉W_1, ⇉W_2)$ |
|:----------------|:-------:|:---:|
| ReLU            | $\max(0, x)$ | $\max(0, →x⇉W_1)⇉W_2$
~~~
| GELU            | $x Φ(x)$ | $\operatorname{GELU}(→x⇉W_1)⇉W_2$
~~~
| Swish           | $x σ(x)$ | $\operatorname{Swish}(→x⇉W_1)⇉W_2$
~~~

There are several variants of the new gated activations:

| Activation Name | Formula | $\operatorname{FFN}(x; ⇉W, ⇉V, ⇉W_2)$ |
|:----------------|:-------:|:---:|
| GLU (Gated Linear Unit) | $σ(→x⇉W + →b)⊙(→x⇉V + →c)$ | $(σ(→x⇉W)⊙→x⇉V)⇉W_2$
~~~
| ReGLU | $\max(0, →x⇉W + →b)⊙(→x⇉V + →c)$ | $(\max(0, →x⇉W)⊙→x⇉V)⇉W_2$
| GEGLU | $\operatorname{GELU}(→x⇉W + →b)⊙(→x⇉V + →c)$ | $(\operatorname{GELU}(→x⇉W)⊙→x⇉V)⇉W_2$
| SwiGLU | $\operatorname{Swish}(→x⇉W + →b)⊙(→x⇉V + →c)$ | $(\operatorname{Swish}(→x⇉W)⊙→x⇉V)⇉W_2$

---
# Gated Activations in Transformers

![w=73.8%](gated_transformers_glue.svgz)![w=26.2%](gated_transformers_squad.svgz)

![w=100%](gated_transformers.svgz)

---
section: ParallelWaveNet
style: .katex-display { margin: .8em 0 }
# Parallel WaveNet

Parallel WaveNet is an improvement of the original WaveNet by the same authors.

~~~
First, the output distribution was changed from 256 $μ$-law values to a Mixture of
Logistic (suggested in another paper – PixelCNN++, but reused in other architectures since):
$$x ∼ ∑_i π_i \operatorname{Logistic}(μ_i, s_i).$$

~~~
![w=28%,f=right](logistic_pdf.svgz)

The logistic distribution is a distribution with a $σ$ as cumulative density function
(where the mean and scale is parametrized by $μ$ and $s$).
~~~
Therefore, we can write
$$P(x | →π, →μ, →s) = ∑_i π_i \bigg[σ\Big(\frac{x + 0.5 - μ_i}{s_i}\Big) - σ\Big(\frac{x - 0.5 - μ_i}{s_i}\Big)\bigg],$$
where we replace $-0.5$ and $0.5$ in the edge cases by $-∞$ and $∞$.

~~~
In Parallel WaveNet teacher, 10 mixture components are used.

---
style: .katex-display { margin: .8em 0 }
# Parallel WaveNet

Auto-regressive (sequential) inference is extremely slow in WaveNet.

~~~
Instead, we model $P(x_t)$ as $P(x_t | →z_{<t})
= \operatorname{Logistic}\big(x_t; μ^1(→z_{< t}), s^1(→z_{< t})\big)$
for a _random_ $→z$ drawn from a logistic distribution
$\operatorname{Logistic}(→0, →1)$. Therefore, using the reparametrization
trick,
$$x^1_t = μ^1(→z_{< t}) + z_t ⋅ s^1(→z_{< t}).$$

~~~
Usually, one iteration of the algorithm does not produce good enough results
– consequently, 4 iterations were used by the authors. In further iterations,
$$x^i_t = μ^i(→x^{i-1}_{< t}) + x^{i-1}_t ⋅ s^i(→x^{i-1}_{< t}).$$

~~~
After $N$ iterations, $P(→x^N_t | →z_{<t})$ is a logistic distribution
with location $→μ^\textrm{tot}$ and scale $→s^\textrm{tot}$:
$$μ^\textrm{tot}_t = ∑_{i=1}^N μ^i(→x^{i-1}_{< t}) ⋅ \Big(∏\nolimits_{j>i}^N s^j(→x^{j-1}_{< t})\Big) \textrm{~~and~~}
  s^\textrm{tot}_t = ∏_{i=1}^N s^i(→x^{i-1}_{< t}),$$
where we have denoted $→z$ as $→x^0$ for convenience.

---
style: .katex-display { margin: .8em 0 }
# Parallel WaveNet

The consequences of changing the model to

$$\begin{aligned}
x^1_t &= μ^1(→z_{< t}) + z_t ⋅ s^1(→z_{< t}) \\
x^i_t &= μ^i(→x^{i-1}_{< t}) + x^{i-1}_t ⋅ s^i(→x^{i-1}_{< t}) \\
\end{aligned}$$

are:
~~~
- During inference, the prediction can be computed in parallel, because
  $x^i_t$ depends only on $→x^{i-1}_{< t}$, not on $x^i_{< t}$.
~~~
- However, we cannot perform training in parallel.
~~~
  If we try maximizing the log-likelihood of an input sequence $→x^1$,
  we need to find out which $→z$ sequence generates it.
~~~
  - The $z_1$ can be computed using $x^1_1$.
~~~
  - However, $z_2$ depends on only on $x^1_1$ and $x^1_2$, but also on $z_1$;
    generally, $z_t$ depends on $→x^1$ and also on all $→z_{< t}$, and can be
    computed only sequentially.

~~~
Therefore, WaveNet can perform parallel training and sequential inference, while
the proposed model can perform parallel inference but sequential training.

---
# Probability Density Distillation

The authors propose to train the network by a **probability density distillation** using
a teacher WaveNet (producing a mixture of logistic with 10 components) with KL-divergence as a loss.

![w=75%,h=center](parallel_wavenet_distillation.svgz)

---
style: .katex-display { margin: .8em 0 }
# Probability Density Distillation

Therefore, instead of computing $→z$ from some gold $→x_g$, we
~~~
- sample a random $→z$;
~~~
- generate the output $→x$;
~~~
- use the teacher WaveNet model to estimate the log-likelihood
  of $→x$;
~~~
- update the student to match the log-likelihood of the teacher.

~~~
Denoting the teacher distribution as $P_T$ and the student distribution
as $P_S$, the loss is
$$D_\textrm{KL}(P_S || P_T) = H(P_S, P_T) - H(P_S).$$

~~~
Therefore, we do not only minimize cross-entropy, but we also try to keep the
entropy of the student as high as possible – it is indeed crucial not to match
just the mode of the teacher.
~~~
- Consider a teacher generating white noise, where every sample comes from
  $𝓝(0, 1)$ – in this case, the cross-entropy loss of a constant $→0$, complete
  silence, would be maximal.

~~~
In a sense, probability density distillation is similar to GANs. However,
the teacher is kept fixed, and the student does not attempt to fool it
but to match its distribution instead.

---
# Probability Density Distillation Details

Because the entropy of a logistic distribution $\operatorname{Logistic}(μ, s)$
is $\log s + 2$, the entropy term $H(P_S)$ can be rewritten as follows:
$$\begin{aligned}
H(P_S) &= 𝔼_{z∼\operatorname{Logistic}(0, 1)}\left[\sum_{t=1}^T - \log p_S(x_t|→z_{<t})\right] \\
       &= 𝔼_{z∼\operatorname{Logistic}(0, 1)}\left[\sum_{t=1}^T \log s(→z_{\lt t}, →θ)\right] + 2T.
\end{aligned}$$
Therefore, this term can be computed without having to generate $→x$.

---
# Probability Density Distillation Details

However, the cross-entropy term $H(P_S, P_T)$ requires sampling from $P_S$ to estimate:

~~~
$\displaystyle \kern6em{}\mathllap{H(P_S, P_T)} = ∫_{→x} -P_S(→x) \log P_T(→x)$

~~~
$\displaystyle \kern6em{} = ∑_{t=1}^T ∫_{→x} -P_S(→x) \log P_T(x_t|→x_{<t})$

~~~
$\displaystyle \kern6em{} = ∑_{t=1}^T ∫_{→x} -\textcolor{blue}{P_S(→x_{<t})}\textcolor{green}{P_S(x_t|→x_{<t})}\textcolor{red}{P_S(→x_{>t}|→x_{\leq t})} \log P_T(x_t|→x_{<t})$

~~~
$\displaystyle \kern6em{} = ∑_{t=1}^T 𝔼_{\textcolor{blue}{P_S(→x_{<t})}} \bigg[∫_{x_t} -\textcolor{green}{P_S(x_t|→x_{<t})} \log P_T(x_t|→x_{<t}) ∫_{→x_{>t}} \textcolor{red}{P_S(→x_{>t}|→x_{\leq t})}\bigg]$

~~~ ~~
$\displaystyle \kern6em{} = ∑_{t=1}^T 𝔼_{\textcolor{blue}{P_S(→x_{<t})}} \bigg[∫_{x_t} -\textcolor{green}{P_S(x_t|→x_{<t})} \log P_T(x_t|→x_{<t}) \underbrace{\,∫_{→x_{>t}} \textcolor{red}{P_S(→x_{>t}|→x_{\leq t})}}_1\bigg]$

~~~
$\displaystyle \kern6em{} = ∑_{t=1}^T 𝔼_{P_S(→x_{<t})} H\Big(P_S(x_t|→x_{<t}), P_T(x_t|→x_{<t})\Big).$

---
# Probability Density Distillation Details

$$H(P_S, P_T) = ∑_{t=1}^T 𝔼_{P_S(→x_{<t})} H\Big(P_S(x_t|→x_{<t}), P_T(x_t|→x_{<t})\Big)$$

We can therefore estimate $H(P_S, P_T)$ by:
- drawing a single sample $→x$ from the student $P_S$ _[a Logistic($→μ^\textrm{tot}, →s^\textrm{tot}$)]_,
~~~
- compute all $P_T(x_t | →x_{<t})$ from the teacher in parallel _[mixture of
  logistic distributions]_,
~~~
- and finally evaluate $H(P_S(x_t|→x_{<t}), P_T(x_t|→x_{<t}))$ by sampling
  multiple different $x_t$ from the $P_S(x_t|→x_{<t})$.

~~~
The authors state that this unbiased estimator has a much lower variance than
naively evaluating a single sequence sample under the teacher using the original formulation.

~~~
Finally, analogously to the normal distribution, the logistic distribution
offers the **reparametrization trick**.
~~~
Therefore, we can differentiate $\log P_T(x_t|→x_{<t})$ with respect to
both $x_t$ and $→x_{<t}$ (while the categorical distribution is differentiable
only with respect to $→x_{< t}$).

---
# Parallel WaveNet

With the 4 iterations, the Parallel WaveNet generates over 500k samples per
second, compared to ~170 samples per second of a regular WaveNet – more than
a 1000 times speedup.

![w=80%,mh=80%,v=bottom,h=center](parallel_wavenet_results.svgz)

---
class: dbend
style: .katex-display { margin: .4em 0 }
# Parallel WaveNet – Additional Losses

To generate high-quality audio, the probability density distillation is not
entirely sufficient. The authors therefore introduce additional losses:
~~~
- **power loss**: ensures the power in different frequency bands is on average
  similar between the generated speech and human speech. For a conditioned
  training data $(→x, →c)$ and WaveNet student $g$, the loss is
  $$\big\|\operatorname{STFT}(g(→z, →c)) - \operatorname{STFT}(→x)\big\|^2.$$
~~~
- **perceptual loss**: apart from the power in frequency bands, we can use
  a pre-trained classifier to extract features from generated and human speech
  and add a loss measuring their difference. The authors propose the loss as
  squared Frobenius norm of differences between Gram matrices (uncentered
  covariance matrices) of features of a WaveNet-like classifier predicting
  phones from raw audio.
~~~
- **contrastive loss**: to make the model respect the conditioning instead of
  generating outputs with high likelihood independent on the conditioning,
  the authors propose a contrastive distillation loss ($γ=0.3$ is used in the
  paper):
  $$D_\textrm{KL}\big(P_S(→c_1) || P_T(→c_1)\big) - γ D_\textrm{KL}\big(P_S(→c_1) || P_T(→c_2)\big).$$

---
class: dbend
# Parallel WaveNet – Additional Losses

![w=100%](parallel_wavenet_losses.svgz)

---
section: Tacotron
# Tacotron 2

Tacotron 2 model presents end-to-end speech synthesis directly from text.
~~~
It consists of two components trained separately:
- a seq2seq model processing input characters and generating mel spectrograms;
- a Parallel WaveNet generating the speech from Mel spectrograms.

![w=48%,h=center](tacotron.svgz)

---
style: .katex-display { margin: .6em 0 }
# Tacotron 2

The Mel spectrograms are computed using STFT (short-time Fourier transform).

~~~
- The authors propose a frame size of 50ms, 12.5ms frame hop, and a Hann window.

~~~
- STFT magnitudes are transformed into 80-channel Mel scale spanning 175Hz
  to 7.6kHz, followed by a log dynamic range compression (clipping input values
  to at least 0.01).

~~~
To make sequential processing of input characters easier, Tacotron 2 utilizes
_location-sensitive attention_, which is an extension of the additive attention.
While the additive (Bahdanau) attention computes
$$→α_i = \mathrm{Attend}(→s_{i-1}, →h),\textrm{~~~~}α_{ij} = \softmax\big(→v^\top \tanh(⇉V→h_j + ⇉W→s_{i-1} + →b)\big),$$

~~~
the location-sensitive attention also inputs the previous time step attention
weights into the current attention computation:
$$→α_i = \mathrm{Attend}(→s_{i-1}, →h, →α_{i-1}).$$

~~~
In detail, the previous attention weights are processed by a 1-D convolution with kernel $⇉F$:
$$α_{ij} = \softmax\big(→v^\top \tanh(⇉V→h_j + ⇉W→s_{i-1} + (⇉F * →α_{i-1})_j + →b)\big).$$

---
# Tacotron 2

![w=65%,h=center,v=middle](tacotron_results.svgz)

---
class: middle
# Tacotron 2

![w=100%](tacotron_comparison.png)

You can listen to samples at https://google.github.io/tacotron/publications/tacotron2/

---
# Tacotron 2

![w=49%](tacotron_spectrograms.svgz)
~~~
![w=49%](tacotron_vocoders.svgz)
~~~

![w=70%,h=center](tacotron_wavenet_sizes.svgz)

---
section: NTM
# Neural Turing Machines

So far, all input information was stored either directly in network weights, or
in a state of a recurrent network.

~~~
However, mammal brains seem to operate with a **working memory** – a capacity for
short-term storage of information and its rule-based manipulation.

~~~
We can therefore try to introduce an external memory to a neural network. The
memory $⇉M$ will be a matrix, where rows correspond to memory cells.

---
# Neural Turing Machines

The network will control the memory using a controller which reads from the
memory and writes to is. Although the original paper also considered
a feed-forward (non-recurrent) controller, usually the controller is a recurrent
LSTM network.

![w=55%,h=center](ntm_architecture.svgz)

---
# Neural Turing Machine

## Reading

To read the memory in a differentiable way, the controller at time $t$ emits
a read distribution $→w_t$ over memory locations, and the returned read vector $→r_t$
is then
$$→r_t = ∑_i w_t(i) ⋅ →M_t(i).$$

~~~
## Writing

Writing is performed in two steps – an **erase** followed by an **add**: the
controller at time $t$ emits a write distribution $→w_t$ over memory locations,
together with an _erase vector_ $→e_t$ and an _add vector_ $→a_t$. The memory is then
updated as
$$→M_t(i) = →M_{t-1}(i)\big[1 - w_t(i)→e_t] + w_t(i) →a_t.$$

---
# Neural Turing Machine

The addressing mechanism is designed to allow both
- content addressing, and
- location addressing.

![w=90%,h=center](ntm_addressing.svgz)

---
# Neural Turing Machine

## Content Addressing

Content addressing starts by the controller emitting the _key vector_ $→k_t$,
which is compared to all memory locations $→M_t(i)$, generating a distribution
using a $\softmax$ with temperature $β_t$.
$$w_t^c(i) = \frac{\exp(β_t ⋅ \operatorname{distance}(→k_t, →M_t(i))}{∑_j \exp(β_t ⋅ \operatorname{distance}(→k_t, →M_t(j))}$$

~~~
The $\operatorname{distance}$ measure is usually the cosine similarity
$$\operatorname{distance}(→a, →b) = \frac{→a^T →b}{||→a|| ⋅ ||→b||}.$$

---
# Neural Turing Machine

## Location-Based Addressing

To allow iterative access to memory, the controller might decide to reuse the
memory location from the previous timestep. Specifically, the controller emits
an _interpolation gate_ $g_t$ and sets
$$→w_t^g = g_t →w_t^c + (1 - g_t) →w_{t-1}.$$

~~~
Then, the current weighting may be shifted, i.e., the controller might decide to
“rotate” the weights by a small integer. For a given range (the simplest case
are only shifts $\{-1, 0, 1\}$), the network emits a $\softmax$ distribution over
the shifts, and the weights are then defined using a circular convolution
$$w̃_t(i) = ∑_j w_t^g(j) s_t(i - j).$$

~~~
Finally, not to lose precision over time, the controller emits
a _sharpening factor_ $γ_t$, and the final memory location weights are
$w_t(i) = {w̃_t(i)^{γ_t}} / {∑_j w̃_t(j)^{γ_t}}.$

---
# Neural Turing Machine

## Overall Execution

Even if not specified in the original paper, following the DNC paper, the LSTM
controller can be implemented as a (potentially deep) LSTM. Assuming $R$ read
heads and one write head, the input is $→x_t$ and $R$ read
vectors $→r_{t-1}^1, …, →r_{t-1}^R$ from the previous time step, the output of the
controller are vectors $(→ν_t, →ξ_t)$, and the final output is
$→y_t = →ν_t + ⇉W_r\big[→r_t^1, …, →r_t^R\big]$. The $→ξ_t$ is a concatenation of
$$→k_t^1, β_t^1, g_t^1, →s_t^1, γ_t^1, →k_t^2, β_t^2, g_t^2, →s_t^2, γ_t^2, …, →k_t^w, β_t^w, g_t^w, →s_t^w, γ_t^w, →e_t^w, →a_t^w.$$

---
# Neural Turing Machines

## Copy Task

Repeat the same sequence as given on input. Trained with sequences of length up
to 20.

![w=70%,h=center](ntm_copy_training.svgz)

---
# Neural Turing Machines

![w=84%,h=center](ntm_copy_generalization.svgz)

---
# Neural Turing Machines

![w=95%,h=center](ntm_copy_generalization_lstm.svgz)

---
# Neural Turing Machines

![w=65%,h=center](ntm_copy_memory.svgz)

---
# Neural Turing Machines

## Associative Recall

In associative recall, a sequence is given on input, consisting of subsequences
of length 3. Then a randomly chosen subsequence is presented on input and the
goal is to produce the following subsequence.

![w=65%,h=center](ntm_associative_recall_training.svgz)

---
# Neural Turing Machines

![w=83%,h=center](ntm_associative_recall_generalization.svgz)

---
# Neural Turing Machines

![w=53%,h=center](ntm_associative_recall_memory.svgz)

---
section: DNC
# Differentiable Neural Computer

NTM was later extended to a Differentiable Neural Computer.

![w=82%,h=center](dnc_architecture.svgz)

---
# Differentiable Neural Computer

The DNC contains multiple read heads and one write head.

~~~
The controller is a deep LSTM network, with input at time $t$ being the current
input $→x_t$ and $R$ read vectors $→r_{t-1}^1, …, →r_{t-1}^R$ from previous time
step. The output of the controller are vectors $(→ν_t, →ξ_t)$, and the final
output is $→y_t = →ν_t + W_r\big[→r_t^1, …, →r_t^R\big]$. The $→ξ_t$ is
a concatenation of parameters for read and write heads (keys, gates, sharpening
parameters, …).

~~~
In DNC, the usage of every memory location is tracked, which enables performing
dynamic allocation – at each time step, a cell with least usage can be allocated.

~~~
Furthermore, for every memory location, we track which memory location was
written to previously and subsequently, allowing to recover sequences in the
order in which it was written, independently on the real indices used.

~~~
The write weighting is defined as a weighted combination of the allocation
weighting and write content weighting, and read weighting is computed as a weighted
combination of read content weighting, previous write weighting, and subsequent
write weighting.


---
# Differentiable Neural Computer

![w=100%,v=middle](dnc_graph_tasks.svgz)

---
# Differentiable Neural Computer

![w=100%,v=middle](dnc_graph_tasks_traversal.svgz)

---
section: MANN
# Memory-augmented Neural Networks

External memory can be also utilized for **learning to learn**. Consider a network,
which should learn classification into a user-defined hierarchy by observing
ideally a small number of samples.

~~~
Apart from finetuning the model and storing the information in the _weights_,
an alternative is to store the samples in **external memory**. Therefore, the
model learns how to store the data and access it efficiently, which allows
it to learn without changing its weights.

![w=100%](mann_overview.svgz)

---
# Memory-augmented NNs

![w=90%,mw=62%](mann_reading.svgz)![w=38%](mann_writing.svgz)

---
# Memory-augmented NNs

![w=60%,h=center](mann_results.svgz)

---
# Memory-augmented NNs

![w=45%,h=center](mann_results_5.svgz)

![w=95%,h=center](mann_results_15.svgz)
